# Computer Vision

> Including **Classification**, **Recognition**, **Detection**, **Captioning**, **GAN** and etc.

## Datasets
- [2018 CVPR] **Referring Relationships**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Krishna_Referring_Relationships_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Referring%20Relationships.bib), [[homepage]](https://cs.stanford.edu/people/ranjaykrishna/referringrelationships/), sources: [[StanfordVL/ReferringRelationships]](https://github.com/StanfordVL/ReferringRelationships).
- [2019 CVPR] **From Recognition to Cognition: Visual Commonsense Reasoning**, [[paper]](https://arxiv.org/pdf/1811.10830.pdf), [[bibtex]](/Bibtex/From%20Recognition%20to%20Cognition%20-%20Visual%20Commonsense%20Reasoning.bib), [[homepage]](https://visualcommonsense.com), [[leaderboard]](https://visualcommonsense.com/leaderboard/), [[dataset]](https://visualcommonsense.com/download/), sources: [[rowanz/r2c]](https://github.com/rowanz/r2c/).

## Image Processing
- [2018 IJCAI] **DehazeGAN: When Image Dehazing Meets Differential Programming**, [[paper]](http://www.ijcai.org/proceedings/2018/0172.pdf).
- [2018 AAAI] **Towards Perceptual Image Dehazing by Physics-based Disentanglement and Adversarial Training**, [[paper]](https://pdfs.semanticscholar.org/7a73/6b46b37a67a440a29593e261f7c0b63f0ad5.pdf).

## Image Classification/Recognition and Object Detection
- [2012 NIPS] **ImageNet Classification with Deep Convolutional Neural Networks**, _AlexNet_, [[paper]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).
- [2014 ArXiv] **Very Deep Convolutional Networks for Large-Scale Image Recognition**, _VGG_, [[paper]](https://arxiv.org/abs/1409.1556.pdf).
- [2015 CVPR] **Going Deeper with Convolutions**, _GoogLeNet_, [[paper]](https://arxiv.org/abs/1409.4842.pdf).
- [2015 ICCV] **Learning Spatiotemporal Features with 3D Convolutional Networks**, [[paper]](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf), [[bibtex]](/Bibtex/Learning%20Spatiotemporal%20Features%20with%203D%20Convolutional%20Networks.bib), [[homepage]](http://vlg.cs.dartmouth.edu/c3d/), sources: [[facebook/C3D]](https://github.com/facebook/C3D), [[hx173149/C3D-tensorflow]](https://github.com/hx173149/C3D-tensorflow).
- [2015 ICML] **Siamese Neural Networks for One-shot Image Recognition**, [[paper]](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf), sources: [[Goldesel23/Siamese-Networks-for-One-Shot-Learning]](https://github.com/Goldesel23/Siamese-Networks-for-One-Shot-Learning).
- [2017 CVPR] **Feature Pyramid Networks for Object Detection**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.pdf), [[bibtex]](/Bibtex/Feature%20Pyramid%20Networks%20for%20Object%20Detection.bib), sources: [[unsky/FPN]](https://github.com/unsky/FPN), [[DetectionTeamUCAS/FPN_Tensorflow]](https://github.com/DetectionTeamUCAS/FPN_Tensorflow), [[yangxue0827/FPN_Tensorflow]](https://github.com/yangxue0827/FPN_Tensorflow).
- [2017 ICCV] **Mask R-CNN**, [[paper]](https://arxiv.org/pdf/1703.06870.pdf), [[tutorial]](http://kaiminghe.com/iccv17tutorial/maskrcnn_iccv2017_tutorial_kaiminghe.pdf), [[video]](https://www.youtube.com/watch?v=2TikTv6PWDw), sources: [[matterport/Mask_RCNN]](https://github.com/matterport/Mask_RCNN), [[CharlesShang/FastMaskRCNN]](https://github.com/CharlesShang/FastMaskRCNN), [[facebookresearch/Detectron]](https://github.com/facebookresearch/Detectron).
- [2017 ICCV] **Focal Loss for Dense Object Detection**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/Focal%20Loss%20for%20Dense%20Object%20Detection.bib), sources: [[unsky/focal-loss]](https://github.com/unsky/focal-loss), [[ailias/Focal-Loss-implement-on-Tensorflow]](https://github.com/ailias/Focal-Loss-implement-on-Tensorflow).
- [2017 CVPR] **Densely Connected Convolutional Networks**, [[paper]](https://arxiv.org/abs/1608.06993.pdf), sources: [[liuzhuang13/DenseNet]](https://github.com/liuzhuang13/DenseNet).
- [2018 CVPR] **Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs**, [[paper]](https://arxiv.org/pdf/1803.08035.pdf), [[blog]](https://www.cnblogs.com/wangxiaocvpr/p/8682608.html), sources: [[JudyYe/zero-shot-gcn]](https://github.com/JudyYe/zero-shot-gcn).
- [2018 ECCV] **DOCK: Detecting Objects by Transferring Common-sense Knowledge**, [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/papers/Krishna_Kumar_Singh_Transferring_Common-Sense_Knowledge_ECCV_2018_paper.pdf), [[bibtex]](/Bibtex/DOCK%20-%20Detecting%20Objects%20by%20transferring%20Common-sense%20Knowledge.bib), sources: [[kkanshul/dock]](https://github.com/kkanshul/dock).

## Instance/Semantic Description/Segmentation
- [2015 IPMI] **Predicting Semantic Descriptions from Medical Images with Convolutional Neural Networks**, [[paper]](/Documents/Papers/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.pdf), [[bibtex]](/Bibtex/Predicting%20Semantic%20Descriptions%20from%20Medical%20Images%20with%20Convolutional%20Neural%20Networks.bib).
- [2017 CVPR] **PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation**, [[paper]](https://arxiv.org/pdf/1612.00593.pdf), sources: [[charlesq34/pointnet]](https://github.com/charlesq34/pointnet).
- [2018 ICML] **Attention-based Deep Multiple Instance Learning**, [[paper]](https://arxiv.org/pdf/1802.04712.pdf), [[bibtex]](/Bibtex/Attention-based%20Deep%20Multiple%20Instance%20Learning.bib), sources: [[AMLab-Amsterdam/AttentionDeepMIL]](https://github.com/AMLab-Amsterdam/AttentionDeepMIL).

## Image/Video Captioning
- [2015 ICML] **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention**, [[paper]](https://arxiv.org/pdf/1502.03044.pdf), [[slides]](http://www.cs.toronto.edu/~fidler/slides/2017/CSC2539/Katherine_slides.pdf), [[bibtex]](/Bibtex/Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention.bib),  [[homepage]](http://kelvinxu.github.io/projects/capgen.html), sources: [[kelvinxu/arctic-captions]](https://github.com/kelvinxu/arctic-captions), [[yunjey/show-attend-and-tell]](https://github.com/yunjey/show-attend-and-tell), [[DeepRNN/image_captioning]](https://github.com/DeepRNN/image_captioning), [[coldmanck/show-attend-and-tell]](https://github.com/coldmanck/show-attend-and-tell).
- [2017 PAMI] **Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge**, [[paper]](https://arxiv.org/abs/1609.06647.pdf), sources: [[tensorflow/models/im2txt]](https://github.com/tensorflow/models/tree/master/research/im2txt).
- [2018 ACL] **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning**, [[paper]](http://aclweb.org/anthology/P18-1238), [[bibtex]](/Bibtex/Conceptual%20Captions%20-%20A%20Cleaned%20Hypernymed%20Image%20Alt-text%20Dataset%20For%20Automatic%20Image%20Captioning.bib), [[homepage]](https://ai.google.com/research/ConceptualCaptions), sources: [[google-research-datasets/conceptual-captions]](https://github.com/google-research-datasets/conceptual-captions).

## Action Recognition
- [2016 CVPR] **Dynamic Image Networks for Action Recognition**, [[paper]](https://www.egavves.com/data/cvpr2016bilen.pdf), [[bibtex]](/Bibtex/Dynamic%20Image%20Networks%20for%20Action%20Recognition.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).
- [2016 CVPR] **End-to-end Learning of Action Detection from Frame Glimpses in Videos**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2016/papers/Yeung_End-To-End_Learning_of_CVPR_2016_paper.pdf), sources: [[syyeung/frameglimpses]](https://github.com/syyeung/frameglimpses).
- [2017 PAMI] **Action Recognition with Dynamic Image Networks**, [[paper]](http://homepages.inf.ed.ac.uk/hbilen/assets/pdf/Bilen17a.pdf), [[bibtex]](/Bibtex/Action%20Recognition%20with%20Dynamic%20Image%20Networks.bib), sources: [[hbilen/dynamic-image-nets]](https://github.com/hbilen/dynamic-image-nets).

## Generative Adversarial Network (GAN)
- [2014 NIPS] **Generative Adversarial Nets**, [[paper]](https://arxiv.org/abs/1406.2661), sources: [[goodfeli/adversarial]](https://github.com/goodfeli/adversarial), [[aymericdamien/TensorFlow-Examples]](https://github.com/aymericdamien/TensorFlow-Examples).
- [2016 ICLR] **Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks**, [[paper]](https://arxiv.org/abs/1511.06434), sources: [[Newmu/dcgan_code]](https://github.com/Newmu/dcgan_code).
- [2017 ICML] **Wasserstein Generative Adversarial Networks**, [[paper]](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf), [[supplementary]](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a-supp.pdf), [[bibtex]](/Bibtex/Wasserstein%20Generative%20Adversarial%20Networks.bib), [[homepage]](http://proceedings.mlr.press/v70/arjovsky17a.html), sources: [[kpandey008/wasserstein-gans]](https://github.com/kpandey008/wasserstein-gans), [[martinarjovsky/WassersteinGAN]](https://github.com/martinarjovsky/WassersteinGAN), [[luslab/scRNAseq-WGAN-GP]](https://github.com/luslab/scRNAseq-WGAN-GP).
- [2017 AAAI] **SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient**, [[paper]](https://arxiv.org/abs/1609.05473), sources:[[LantaoYu/SeqGAN]](https://github.com/LantaoYu/SeqGAN).
- [2017 ICCV] **CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks**, [[paper]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf), [[bibtex]](/Bibtex/Unpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks.bib), [[homepage]](https://junyanz.github.io/CycleGAN/), sources: [[junyanz/pytorch-CycleGAN-and-pix2pix]](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix), [[junyanz/CycleGAN]](https://github.com/junyanz/CycleGAN), [[xhujoy/CycleGAN-tensorflow]](https://github.com/xhujoy/CycleGAN-tensorflow).
- [2018 ICLR] **CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training**, [[paper]](https://openreview.net/pdf?id=BJE-4xW0W), [[bibtex]](/Bibtex/CausalGAN%20-%20Learning%20Causal%20Implicit%20Generative%20Models%20with%20Adversarial%20Training.bib), sources: [[mkocaoglu/CausalGAN]](https://github.com/mkocaoglu/CausalGAN).
- [2018 ICLR] **Progressive Growing of GANs for Improved Quality, Stability, and Variation**, [[paper]](https://openreview.net/pdf?id=Hk99zCeAb), [[bibtex]](/Bibtex/Progressive%20Growing%20of%20GANs%20for%20Improved%20Quality%20Stability%20and%20Variation.bib), sources: [[tkarras/progressive_growing_of_gans]](https://github.com/tkarras/progressive_growing_of_gans).
- [2018 ArXiv] **A Style-Based Generator Architecture for Generative Adversarial Networks**, [[paper]](https://arxiv.org/pdf/1812.04948.pdf), [[bibtex]](/Bibtex/A%20Style-Based%20Generator%20Architecture%20for%20Generative%20Adversarial%20Networks.bib), [[blog]](https://towardsdatascience.com/explained-a-style-based-generator-architecture-for-gans-generating-and-tuning-realistic-6cb2be0f431), sources: [[NVlabs/stylegan]](https://github.com/NVlabs/stylegan), [[Puzer/stylegan-encoder]](https://github.com/Puzer/stylegan-encoder).
- [2018 ArXiv] **XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings**, [[paper]](https://arxiv.org/pdf/1711.05139.pdf), [[bibtex]](/Bibtex/XGAN%20-%20Unsupervised%20Image-to-Image%20Translation%20for%20Many-to-Many%20Mappings.bib), [[Cartoon Dataset]](https://google.github.io/cartoonset/index.html).
- [2018 ICLR] **Spectral Normalization for Generative Adversarial Networks**, [[paper]](https://openreview.net/pdf?id=B1QRgziT-), [[bibtex]](/Bibtex/Spectral%20Normalization%20for%20Generative%20Adversarial%20Networks.bib), sources: [[pfnet-research/sngan_projection]](https://github.com/pfnet-research/sngan_projection).

## Multi-tasks and Multi-model
- [2018 ICLR] **Beyond Shared Hierarchies: Deep Multitask Learning Through Soft Layer Ordering**, [[paper]](https://openreview.net/pdf?id=BkXmYfbAZ), [[bibtex]](/Bibtex/Beyond%20Shared%20Hierarchies%20-%20Deep%20Multitask%20Learning%20Through%20Soft%20Layer%20Ordering.bib).

## Unassorted Research Works
- [2014 ECCV] **Visualizing and Understanding Convolutional Networks**, [[paper]](https://arxiv.org/abs/1311.2901.pdf).
- [2016 NIPS] **Matching Networks for One Shot Learning**, [[paper]](https://arxiv.org/pdf/1606.04080.pdf), source: [[AntreasAntoniou/MatchingNetworks]](https://github.com/AntreasAntoniou/MatchingNetworks), [[BoyuanJiang/matching-networks-pytorch]](https://github.com/BoyuanJiang/matching-networks-pytorch).
- [2017 ICLR] **Adversarially Learned Inference**, [[paper]](https://openreview.net/pdf?id=B1ElR4cgg), [[bibtex]](/Bibtex/Adversarially%20Learned%20Inference.bib), sources: [[IshmaelBelghazi/ALI]](https://github.com/IshmaelBelghazi/ALI), [[otenim/ALI-Keras2]](https://github.com/otenim/ALI-Keras2), [[edgarriba/ali-pytorch]](https://github.com/edgarriba/ali-pytorch).
- [2018 CVPR] **Focal Visual-Text Attention for Visual Question Answering**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liang_Focal_Visual-Text_Attention_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Visual%20Question%20Answering.bib), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).
- [2019 TPAMI] **Focal Visual-Text Attention for Memex Question Answering**, [[paper]](http://llcao.net/paper/MemexQA_TPAMI.pdf), [[bibtex]](/Bibtex/Focal%20Visual-Text%20Attention%20for%20Memex%20Question%20Answering.bib), [[homepage]](https://memexqa.cs.cmu.edu), sources: [[JunweiLiang/FVTA_memoryqa]](https://github.com/JunweiLiang/FVTA_memoryqa).