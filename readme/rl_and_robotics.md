## Reinforencement Learning and Robotic Systems

## Reinforencement Learning
- [Key Papers in Deep RL](https://spinningup.openai.com/en/latest/spinningup/keypapers.html).
- [2013 NIPS] **Playing Atari with Deep Reinforcement Learning**, [[paper]](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), [[blog]](https://keon.io/deep-q-learning/), sources: [[kuz/DeepMind-Atari-Deep-Q-Learner]](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner), [[ethanscho/dqn-atari-tensorflow]](https://github.com/ethanscho/dqn-atari-tensorflow), [[carpedm20/deep-rl-tensorflow]](https://github.com/carpedm20/deep-rl-tensorflow).
- [2014 ICML] **Deterministic Policy Gradient Algorithms**, [[paper]](http://proceedings.mlr.press/v32/silver14.pdf), [[supplementary]](http://proceedings.mlr.press/v32/silver14-supp.pdf), [[bibtex]](/Bibtex/Deterministic%20Policy%20Gradient%20Algorithms.bib), [[blog]](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html), sources: [[cookbenjamin/DDPG]](https://github.com/cookbenjamin/DDPG).
- [2015 Nature]  **Human-level control through deep reinforcement learning**, [[paper]](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf), [[blog]](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/), sources: [[devsisters/DQN-tensorflow]](https://github.com/devsisters/DQN-tensorflow), [[floodsung/DQN-Atari-Tensorflow]](https://github.com/floodsung/DQN-Atari-Tensorflow).
- [2018 AAAI] **Deep Reinforcement Learning that Matters**, [[paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16669/16677), [[bibtex]](/Bibtex/Deep%20Reinforcement%20Learning%20that%20Matters.bib), sources: [[Breakend/DeepReinforcementLearningThatMatters]](https://github.com/Breakend/DeepReinforcementLearningThatMatters).
- [2019 TICS] **Reinforcement Learning, Fast and Slow**, [[paper]](https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0), [[bibtex]](/Bibtex/Reinforcement%20Learning%20Fast%20and%20Slow.bib).

# Robotic Systems
- [2014 ACL] **Learning Spatial Knowledge for Text to 3D Scene Generation**, [[paper]](https://www.aclweb.org/anthology/D14-1217), [[bibtex]](/Bibtex/Learning%20Spatial%20Knowledge%20for%20Text%20to%203D%20Scene%20Generation.bib).
- [2015 IROS] **Neural Network based Model for Visual-motor Integration Learning of Robotâ€™s Drawing Behavior: Association of a Drawing Motion from a Drawn Image**, [[paper]](/Documents/Papers/Neural%20Network%20based%20Model%20for%20Visual-motor%20Integration%20Learning%20of%20Robots%20Drawing%20Behavior%20-%20Association%20of%20a%20Drawing%20Motion%20from%20a%20Drawn%20Image.pdf), [[bibtex]](/Bibtex/Neural%20Network%20based%20Model%20for%20Visual-motor%20Integration%20Learning%20of%20Robots%20Drawing%20Behavior%20-%20Association%20of%20a%20Drawing%20Motion%20from%20a%20Drawn%20Image.bib).
- [2016 ICRA] **Learning to Generalize 3D Spatial Relationships**, [[paper]](/Documents/Papers/Learning%20to%20Generalize%203D%20Spatial%20Relationships.pdf), [[bibtex]](/Bibtex/Learning%20to%20Generalize%203D%20Spatial%20Relationships.bib).
- [2016 TCDS] **Spatial Concept Acquisition for a Mobile Robot That Integrates Self-Localization and Unsupervised Word Discovery From Spoken Sentences**, [[paper]](https://arxiv.org/pdf/1602.01208.pdf), [[bibtex]](/Bibtex/Spatial%20Concept%20Acquisition.bib).
- [2017 AAAI] **Natural Language Acquisition and Grounding for Embodied Robotic Systems**, [[paper]](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14913/14038), [[bibtex]](/Bibtex/Natural%20Language%20Acquisition%20and%20Grounding%20for%20Embodied%20Robotic%20Systems.bib).
- [2017 PMLR] **Opportunistic Active Learning for Grounding Natural Language Descriptions**, [[paper]](http://proceedings.mlr.press/v78/thomason17a/thomason17a.pdf), [[bibtex]](/Bibtex/Opportunistic%20Active%20Learning%20for%20Grounding%20Natural%20Language%20Descriptions.bib), sources: [[thomason-jesse/perception_classifiers]](https://github.com/thomason-jesse/perception_classifiers/tree/active_learning).
- [2018 ArXiv] **A Learning Framework for High Precision Industrial Assembly**, [[paper]](https://arxiv.org/pdf/1809.08548.pdf), [[bibtex]](/Bibtex/A%20Learning%20Framework%20for%20High%20Precision%20Industrial%20Assembly.bib).
- [2018 AAAI] **Guiding Exploratory Behaviors for Multi-Modal Grounding of Linguistic Descriptions**, [[paper]](https://www.eecs.tufts.edu/~jsinapov/papers/Thomason_AAAI_2018.pdf), [[bibtex]](/Bibtex/Guiding%20Exploratory%20Behaviors%20for%20Multi-Modal%20Grounding%20of%20Linguistic%20Descriptions.bib), sources: [[thomason-jesse/object_exploration]](https://github.com/thomason-jesse/object_exploration).
- [2018 Robotics] **Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision**, [[paper]](http://www.roboticsproceedings.org/rss14/p12.pdf), [[bibtex]](/Bibtex/Learning%20Task-Oriented%20Grasping%20for%20Tool%20Manipulation%20from%20Simulated%20Self-Supervision.bib).
- [2018 ICRA] **Text2Action: Generative Adversarial Synthesis from Language to Action**, [[paper]](https://arxiv.org/pdf/1710.05298.pdf), [[bibtex]](/Bibtex/Text2Action.bib).
- [2018 CoRL] **QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation**, [[paper]](https://arxiv.org/pdf/1806.10293.pdf), [[bibtex]](/Bibtex/QT-Opt.bib), [[homepage]](https://sites.google.com/view/qtopt), sources: [[DeepX-inc/machina]](https://github.com/DeepX-inc/machina).
- [2018 CVPR] **Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments**, [[paper]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.pdf), [[bibtex]](/Bibtex/Vision-and-Language%20Navigation.bib), sources: [[peteanderson80/Matterport3DSimulator]](https://github.com/peteanderson80/Matterport3DSimulator).
- [2018 ICRA] **Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-To-End Learning from Demonstration**, [[paper]](https://arxiv.org/pdf/1707.02920.pdf), [[bibtex]](/Bibtex/Vision-Based%20Multi-Task%20Manipulation%20for%20Inexpensive%20Robots%20Using%20End-To-End%20Learning%20from%20Demonstration.bib).
- [2018 AAAI] **Learning Interpretable Spatial Operations in a Rich 3D Blocks World**, [[paper]](https://arxiv.org/pdf/1712.03463.pdf), [[bibtex]](/Bibtex/Learning%20Interpretable%20Spatial%20Operations%20in%20a%20Rich%203D%20Blocks%20World.bib), [[dataset]](https://groundedlanguage.github.io), sources: [[ybisk/GroundedLanguage]](https://github.com/ybisk/GroundedLanguage).
- [2019 IROS] **Robot Artist Performs Cartoon Style Facial Portrait Painting**, [[paper]](/Documents/Papers/Robot%20Artist%20Performs%20Cartoon%20Style%20Facial%20Portrait%20Painting.pdf), [[bibtex]](/Bibtex/Robot%20Artist%20Performs%20Cartoon%20Style%20Facial%20Portrait%20Painting.bib).
- [2019 CVPR] **Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks**, [[paper]](https://arxiv.org/pdf/1812.07252.pdf), [[bibtex]](/Bibtex/Sim-to-Real%20via%20Sim-to-Sim.bib), [[homepage]](https://sites.google.com/view/rcan/).
- [2019 TII] **Feedback Deep Deterministic Policy Gradient With Fuzzy Reward for Robotic Multiple Peg-in-Hole Assembly Tasks**, [[paper]](/Documents/Papers/Feedback%20Deep%20Deterministic%20Policy%20Gradient%20With%20Fuzzy%20Reward%20for%20Robotic%20Multiple%20Peg-in-Hole%20Assembly%20Tasks.pdf), [[bibtex]](/Bibtex/Feedback%20Deep%20Deterministic%20Policy%20Gradient%20With%20Fuzzy%20Reward%20for%20Robotic%20Multiple%20Peg-in-Hole%20Assembly%20Tasks.bib), sources: [[hzm2016/Peg_in_hole_assembly]](https://github.com/hzm2016/Peg_in_hole_assembly).
